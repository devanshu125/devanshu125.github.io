---
title: "Natural Language Processing - Part 2üìã"
date: 2020-05-04
tags: [Python, Natural Language Processing, nltk]
header:
  image: "/images/nlp_blog/head_photo_nlp.jpg"
excerpt: "Explaining Bag of Words, Tf-IDF and Word2Vec using Natural Language Toolkit (nltk) in Python."
---
Welcome to Part 2 of the Natural Language Processing blog series. Here I will cover the topics Bag of Words, TF-IDF and Word2Vec.

## Bag of Wordsüëú

Here's an image to explain this concept easily -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/bagofwords.jpg" alt="Bag of Words">

Let's say our text data consists of 3 sentences, they are -

1. He is a good boy
2. She is a good girl
3. Boy and girl are good

The first step is cleaning the data. We will use lemmatization in this case, however you are free to use stemming. So, as always first import the packages and libraries required -

```python
  import nltk
  import re
  from nltk.corpus import stopwords
  from nltk.stem import WordNetLemmatizer
```
Next, we need to initialize our sentences list and the lemmatizer.

```python
  lemmatizer = WordNetLemmatizer()
  sentences = ['He is a good boy', 'She is a good girl', 'Boy and girl are good']
```
Now, comes the cleaning part. First, we need to create an empty list named corpus which will store all the sentences after cleaning. Start by iterating over each sentence, replace anything other than alphabets with space using regular expression, apply lemmatization on the words which are not stopwords, join those words to make a new sentence and append that sentence to the empty list created.

```python
  # store all text in this list after cleaning
  corpus = []

  # iterating over all sentences
  for i in range(len(sentences)):

      # replace all punctuations with ' ' using regular expression
      review = re.sub('[^a-zA-Z]', ' ', sentences[i])

      # convert all to lower-case
      review = review.lower()

      # split to get list of words
      review = review.split()

      # list comprehension for lemmatization
      review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]

      # join words to make sentence
      review = ' '.join(review)

      # append to new list
      corpus.append(review)
  ```
  This is how the new list looks like -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/cleaning_bow_output.PNG" alt="Bag of Words output">

To see how the bag of words model works, let's look at the image at the start. First it creates a simple frequency table of the words present in the sentences. It looks like this -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/freq_table.PNG" alt="Frequency table">

Then it will make vectors based on the occurrence of the words in that particular sentence.

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/vect_table.PNG" alt="Vectors table">

In sentence 1, its only good boy, so good and boy get the number 1 which is True and girl gets 0 which is False. Here's how it would like in code -

```python
  from sklearn.feature_extraction.text import CountVectorizer
  cv = CountVectorizer()
  X = cv.fit(corpus).toarray()
```
Here's what X looks like -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/X_bow.PNG" alt="X output">

The disadvantage of this method is that there is not semantic meaning assigned to the words that is, its either 1 or 0. For example, if we were to do sentiment analysis, the word good should have been given more importance than the word boy or girl, but here all three are given the same number that is 1.

## TF-IDFüìÑ

TF stands for Term Frequency and IDF stands for Inverse Document Frequency. The mathematical formula for both of them is as follows -

```python
  TF = (Number of repetitions of words) / (Total number of words in that sentence)
  IDF = log((Number of sentences) / (Number of sentences containing the word))
```
Let us consider the same 3 sentences as above. The same cleaning steps need to be followed as done above and the new sentences stored in the list corpus will look like this -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/cleaning_bow_output.PNG" alt="Cleaning output">

So, let's see the working of TF-IDF. First it will create a frequency table of the words (same as above) -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/freq_table.PNG" alt="Frequency table">

Now, we need to calculate Term Frequency and Inverse Document Frequency -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/tfidf_table.PNG" alt="TF-IDF table">

To calculate TF-IDF, we multiply Term Frequency and Inverse Document Frequency.

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/mult_tfidf.PNG" alt="TF * IDF">

The implementation in Python is as follows -

```python
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer()
X = tfidf.fit_transform(corpus).toarray()
```
Here's what X looks like -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/X_tfidf.PNG" alt="X_tfidf output">

## Word2Vec‚úíÔ∏è

The problem with TF-IDF and Bag of Words is that semantic information is not stored and they give importance to uncommon words. This can lead to over-fitting. Hence we use Word2Vec.

In Word2Vec, each word is represented as a vector with 32 or more dimensions, therefore semantic relationship is preserved. Let's take an example.

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/word2vec_graph.jpg" alt="Word2Vec graph">

Here there are three vectors in 2-D. Since the word Man and Woman are related, their vectors will be closer to each other whereas the word play is not all related to these two, hence it will not be close to those two. Also, it can make relationships like -

> King - Man + Woman = Queen

As King, Queen, Man and Woman are related, this relationship can be seen mathematically in the form of vectors.

I won't go very deep into the mathematical explanation behind Word2Vec, let's start with the implementation in Python.

First we need to import required libraries and packages.

```python
  import nltk
  import re
  from gensim.models import Word2Vec
  from nltk.corpus import stopwords
```
If this snippet shows an error regarding gensim, that means you will have to install it. Go to the Anaconda prompt and type -
```python
  pip install gensim
```
The paragraph which was used in [Part-1](https://devanshu125.github.io/nlp1) is used here.
Now, the first step involves pre-processing the data, that is we need to remove unwanted things like extra spaces, punctuation and numbers from the paragraph and converting everything to lowercase otherwise it will consider He and he as two different words.

```python
  text = re.sub(r'\[[0-9]*\]', ' ', paragraph)
  text = re.sub(r'\s+', ' ', text)
  text = text.lower()
  text = re.sub(r'\d', ' ', text)
  text = re.sub(r'\s+', ' ', text)
```
The output text is -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/txt_out.PNG" alt="Output text">

Next step is to convert text to sentences and modify those sentences by removing stopwords.

```python
sentences = nltk.sent_tokenize(text)

sentences = [nltk.word_tokenize(sentence) for sentence in sentences]

for i in range(len(sentences)):
    sentences[i] = [word for word in sentences[i] if word not in stopwords.words('english')]
```
This is what the output will look like -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/sent_out.PNG" alt="Sentences Output">

Finally we can train our Word2Vec model!

```python
  model = Word2Vec(sentences, min_count=1)
```
Here, min_count=1 means that any word which occurs less than once should not be considered. In this case this parameter is of no use because our data set is very small. Generally Word2Vec is used for large datasets. However, you are free to experiment with that parameter.

To find out vocabularies which have been found out by our Word2Vec model -

```python
  words = model.wv.vocab
```
This is the output -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/vocab_word2vec.PNG" alt="Word2Vec Vocabulary">

To find word vectors for the word "war" -

```python
  vector = model.wv['war']
```
Here's the vector -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/vector.PNG" alt="Word vector for war">

If you observe carefully, it has 100 dimensions.

To find out most similar words to India -

```python
  similar_words = model.wv.most_similar('india')
```
And these words are -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/similar_india.PNG" alt="Words similar to India in the paragraph">

This concludes the two part blog series on Natural Language Processing. Congratulations, if you've made it so far into the series, you are now ready to apply these skills on real world data. I'd say you start with the SMS Spam Classification data on UCI Machine Learning Repository. Thank you for spending so much time reading this blog, I'll keep posting more such blogs consistently, till then happy coding :)
