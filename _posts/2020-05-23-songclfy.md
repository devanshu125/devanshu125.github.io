---
title: "Classifying song genres from audio dataðŸŽ¹"
date: 2020-05-23
tags: [Python, Machine Learning, Music]
header:
  image: "/images/song_genre_blog/song_blog.jpg"
excerpt: "Apply machine learning methods in Python to classify songs into genres."
---

## IntroductionðŸ’¡
Today, our goal is to apply machine learning methods in Python to classify songs into genres.

## Preparing our datasetðŸ”¨
We'll be examining data compiled by a research group known as The Echo Nest. We need to combine two different datasets, to move forward. The first one has track metadata with genre labels, stored in a CSV file and the second one has track metrics like danceability and acousticness on a scale from -1 to 1 with features, stored in a JSON file. So, first we'll create two separate dataframes and the merge them into one.

```python
  import pandas as pd

  # Read in track metadata with genre labels
  tracks = pd.read_csv('datasets/fma-rock-vs-hiphop.csv')

  # Read in track metrics with the features
  echonest_metrics = pd.read_json('datasets/echonest-metrics.json', precise_float=True)

  # Merge the relevant columns of tracks and echonest_metrics
  echo_tracks = echonest_metrics.merge(tracks[['track_id', 'genre_top']], on='track_id')
```
Now, we can inspect our dataframe further by looking at its info and the top 5 records.

<img src="{{ site.url }}{{ site.baseurl }}/images/song_genre_blog/echo_track.PNG" alt="DataFrame">

## Pairwise relationships between continuous variablesðŸ‘«
To keep the model simple, improve interpretability and to speed up our computation time, we want to avoid using variables which have strong correlations with each other (with many features, we run the risk of overfitting).

```python
  # create a correlation matrix
  corr_metrics = echo_tracks.corr()
  corr_metrics.style.background_gradient()
```
I can't show the output here as its pretty wide. You are free to try it out on our own and find the output!

## Normalizing the feature dataðŸ”¢
Since we didn't find any particular strong correlations between our features, we can instead use a commonly used approach to reduce the number of features called **principal component analysis (PCA)**.

It is possible that variance between the genres can be explained by just a few features in the dataset. PCA rotates the data along the axis of highest variance, thus allowing us to determine the relative contribution of each feature of our data towards the variance between classes.

However, since PCA uses the absolute variance of a feature to rotate the data, a feature with broader range of values will overpower and bias the algorithm relative to the other features. To avoid this, first we need to normalize our feature data. A common way to do this is to use standardization, such that all features have a mean = 0 and standard deviation = 1.

```python
features = echo_tracks.drop(['genre_top', 'track_id'], axis=1)

# Define our labels
labels = echo_tracks['genre_top']

# Import the StandardScaler
from sklearn.preprocessing import StandardScaler

# Scale the features and set the values to a new variable
scaler = StandardScaler()
scaled_train_features = scaler.fit_transform(features)
```

## Principal Component Analysis on our scaled dataðŸ“Š
Now, we are ready to use PCA to determine by how much we can reduce the dimensionality of our data. First let's try with **scree-plots**. They display the number of components against the variance explained by each component, sorted in descending order of variance. This can help us make sense of which component explains a sufficient amount of variance in our data. A steep drop from one data point to the next (elbow) is typically used to decide on an appropriate cutoff.

```python
# Import our plotting module, and PCA class
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Get our explained variance ratios from PCA using all features
pca = PCA()
pca.fit(scaled_train_features)
exp_variance = pca.explained_variance_ratio_

# plot the explained variance using a barplot
fig, ax = plt.subplots()
ax.bar(range(pca.n_components_), exp_variance)
ax.set_xlabel('Principal Component #')
```
<img src="{{ site.url }}{{ site.baseurl }}/images/song_genre_blog/scree_plot.PNG" alt="Scree Plot">

As we cannot see a clear elbow in this scree plot, we must look at the **cumulative explained variance plot** to determine how many features are required to explain, say, about 85% of the variance. Once we determine the appropriate number of components, we can perform PCA with that many components, ideally reducing the dimensionality of our data.

```python
# Import numpy
import numpy as np

# Calculate the cumulative explained variance
cum_exp_variance = np.cumsum(exp_variance)

# Plot the cumulative explained variance and draw a dashed line at 0.85.
fig, ax = plt.subplots()
ax.plot(cum_exp_variance)
ax.axhline(y=0.85, linestyle='--')
```
<img src="{{ site.url }}{{ site.baseurl }}/images/song_genre_blog/cv_plot.PNG" alt="cumulative explained variance plot">

As you can see that when n_components is equal to 6, about 85% of the variance can be explained, hence we perform PCA with number of components equal to 6.

```python
# choose the n_components where about 85% of our variance can be explained
n_components = 6

# Perform PCA with the chosen number of components and project data onto components
pca = PCA(n_components, random_state=10)
pca.fit(scaled_train_features)
pca_projection = pca.transform(scaled_train_features)
```

## Model BuildingðŸŒ±
First we need to split the data into train and test subsets. After doing that, we will start by using decision trees.

```python
# Import train_test_split function and Decision tree classifier
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier

# Split our data
train_features, test_features, train_labels, test_labels = train_test_split(pca_projection, labels, random_state=10)

# Train our decision tree
tree = DecisionTreeClassifier(random_state=10)
tree.fit(train_features, train_labels)

# Predict the labels for the test data
pred_labels_tree = tree.predict(test_features)
```
Now, let's compare our decision tree to logistic regression.

```python
from sklearn.linear_model import LogisticRegression

# Train our logistic regression and predict labels for the test set
logreg = LogisticRegression(random_state=10)
logreg.fit(train_features, train_labels)
pred_labels_logit = logreg.predict(test_features)

# Create the classification report for both models
from sklearn.metrics import classification_report
class_rep_tree = classification_report(test_labels, pred_labels_tree)
class_rep_log = classification_report(test_labels, pred_labels_logit)
print("Decision Tree: \n", class_rep_tree)
print("Logistic Regression: \n", class_rep_log)
```
Here's the output -

<img src="{{ site.url }}{{ site.baseurl }}/images/song_genre_blog/first_report.PNG" alt="First Report">

As you can see that Rock songs are fairly well classified, but hip-hop songs are disproportionately misclassified as rock songs. This is because of we have far more data points for the rock classification than for hip-hop, potentially skewing our model's ability to distinguish between classes. This also tells us that most of our model's accuracy is driven by its ability to classify rock songs. To account for this, we need to balance our dataset.

```python
# Subset only the hip-hop tracks, and then only the rock tracks
hop_only = echo_tracks.loc[echo_tracks["genre_top"] == "Hip-Hop"]
rock_only = echo_tracks.loc[echo_tracks["genre_top"] == "Rock"]

# sample the rocks songs to be the same number as there are hip-hop songs
rock_only = rock_only.sample(n=len(hop_only), random_state=10)

# concatenate the dataframes rock_only and hop_only
rock_hop_bal = pd.concat([rock_only, hop_only])

# The features, labels, and pca projection are created for the balanced dataframe
features = rock_hop_bal.drop(['genre_top', 'track_id'], axis=1)
labels = rock_hop_bal['genre_top']
pca_projection = pca.fit_transform(scaler.fit_transform(features))

# Redefine the train and test set with the pca_projection from the balanced data
train_features, test_features, train_labels, test_labels = train_test_split(pca_projection, labels, random_state=10)
```
So, does this method improve model bias? Let's see!

```python
# Train our decision tree on the balanced data
tree = DecisionTreeClassifier(random_state=10)
tree.fit(train_features, train_labels)
pred_labels_tree = tree.predict(test_features)

# Train our logistic regression on the balanced data
logreg = LogisticRegression(random_state=10)
logreg.fit(train_features, train_labels)
pred_labels_logit = logreg.predict(test_features)

# Compare the models
print("Decision Tree: \n", classification_report(test_labels, pred_labels_tree))
print("Logistic Regression: \n", classification_report(test_labels, pred_labels_logit))
```
<img src="{{ site.url }}{{ site.baseurl }}/images/song_genre_blog/second_report.PNG" alt="Second Report">

## Using cross validation to evaluate our modelsðŸŒ´
Wow! Balancing our data has removed certain bias towards the more prevalent class. To get a sense of how well our model is performing, we can apply cross validation. Here, we will use what's known as K-fold CV. It first splits the data into K different, equally sized subsets and iteratively uses each subset as the test set and the rest of the data as train sets.

```python
# Set up our K-fold cross-validation
kf = KFold(n_splits=10, random_state=10)
tree = DecisionTreeClassifier(random_state=10)
logreg = LogisticRegression(random_state=10)

# Train our models using KFold cv
tree_score = cross_val_score(tree, pca_projection, labels, cv=kf)
logit_score = cross_val_score(logreg, pca_projection, labels, cv=kf)

# Print the mean of each array of scores
print("Decision Tree:", np.mean(tree_score), "Logistic Regression:", np.mean(logit_score))
```

## ConclusionðŸ“œ
Here are the results -

1. **Decision Tree = 0.724**
2. **Logistic Regression = 0.775**

So, this is it from my side. You can try different algorithms and other techniques to get better results than this. I'll be back with another article soon, till then happy coding :)
