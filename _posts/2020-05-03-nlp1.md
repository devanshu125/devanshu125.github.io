---
title: "Natural Language Processing - Part 1ðŸ“‹"
date: 2020-05-03
tags: [Python, Natural Language Processing, nltk]
header:
  image: "/images/nlp_blog/head_photo_nlp.jpg"
excerpt: "Explaining tokenization, stemming and lemmatization using Natural Language Toolkit (nltk) in Python."
---

The aim of this two part blog series is to give the reader a basic understanding of Natural Language Processing. Part 1 of this series covers Tokenization, Stemming and Lemmatization and Part 2 includes Bag of Words, TF-IDF and Word2Vec. I would like to thank Krish Naik for an amazing [playlist](https://www.youtube.com/playlist?list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm) on NLP due to which I am writing this blog.

## TokenizationðŸ”—

In simple words, tokenization is the process of breaking down a paragraph into a list of sentences or words. To understand better let's dive into some code!

We will be using the Natural Language Toolkit package in Python, also known as nltk. First we need to import the package and download all the required libraries.

```python
  import nltk
  nltk.download()
```
Here's the paragraph used -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/paragraph.PNG" alt="paragraph used">

For tokenizing sentences, we use the function sent_tokenize and pass the paragraph as an argument.

```python
  sentences = nltk.sent_tokenize(paragraph)
```

For tokenizing words, we use the function word_tokenize and pass the paragraph as an argument.

```python
  words = nltk.word_tokenize(paragraph)
```
This is what the sentences and words lists look like -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/words_sentences.PNG" alt="Output for word and sentence tokenization">

## StemmingðŸ”£

The process of grouping together different forms of a word and converting them into its root word. For example -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/stemming.PNG" alt="stemming example">

To implement this in Python, first import PorterStemmer from nltk.stem and stopwords from nltk.corpus as follows -

```python
  from nltk.stem import PorterStemmer # library used for stemming
  from nltk.corpus import stopwords
```
Here, we imported stopwords as we have to remove certain words from the paragraph because they generally do not affect the analysis. An example of such words include a, an and the. First step in stemming is to convert the paragraph into sentences which has already been done above. Next we initialize the stemmer.

```python
  stemmer = PorterStemmer()
```
Now, we can start with stemming. First we need to iterate over each sentence, extract all the words in that particular sentence, check if the word belongs to the list of stopwords, if it does not belong to that list, then we perform stemming on that word. In the end we can combine all the words on which stemming has been performed and hence the sentence is modified.

```python
  # iterate over each sentence
  for i in range(len(sentences)):

    # extract all the words in that particular sentence
    words_in_sentence = nltk.word_tokenize(sentences[i])

    # we used set here to get only unique stopwords
    words_in_sentence = [stemmer.stem(word) for word in words_in_sentence if word not in set(stopwords.words('english'))]

    # Join them to convert these words to a sentence
    sentences[i] = ' '.join(words_in_sentence)
```
The new list of sentences will look like this -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/stemming_output.PNG" alt="stemming output">

The problem with stemming is that it produces an indeterminate representation of a word which may not have any meaning. Example : Histori.

## LemmatizationðŸ” 

It is a process of grouping together different forms a word which can then be analysed as a single word.

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/lemmatization.PNG" alt="lemmatization example">

First we need to convert the paragraph into a list of sentences which has been done above. Next, import WordNetLemmatizer from nltk.stem and then initialize it.

```python
  from nltk.stem import WordNetLemmatizer
  lemmatizer = WordNetLemmatizer()
```
Now, we can start with lemmatization. First we need to iterate over each sentence, extract all the words in that particular sentence, check if the word belongs to the list of stopwords, if it does not belong to that list, then we perform lemmatization on that word. In the end we can combine all the words on which lemmatization has been performed and hence the sentence is modified.

```python
  # iterate over each sentence
  for i in range(len(sentences)):

    # extract all the words in that particular sentence
    words_in_sentence = nltk.word_tokenize(sentences[i])

    # we used set here to get only unique stopwords
    words_in_sentence = [lemmatizer.lemmatize(word) for word in words_in_sentence if word not in set(stopwords.words('english'))]

    # Join them to convert these words to a sentence
    sentences[i] = ' '.join(words_in_sentence)
```
The new list of sentences will look like this -

<img src="{{ site.url }}{{ site.baseurl }}/images/nlp_blog/lemmatization_output.PNG" alt="lemmatization output">

And as a wise man said -

> "Stemming is poor people's Lemmatization."

So, you need to choose wisely, although lemmatization is preferred but stemming takes lesser time to execute. In some problems stemming is more useful than lemmatization and in some its the other way round.

This is the end of Part 1. I will upload Part 2 of this series soon where I will be discussing bag of words, TF-IDF and Word2Vec. Till then, happy coding :)
