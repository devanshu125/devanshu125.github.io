---
title: "Pima Indians Diabetes Databaseüíâ"
date: 2020-04-24
tags: [Python, Jupyter Notebook, Machine Learning]
header:
  image: "/images/diabetes_blog/diabetes_img.jpg"
excerpt: "Predict the onset of diabetes based on diagnostic measures."
---
Hello world,

Today I'll try to predict the onset of diabetes based on diagnostic measures. This blog will be divided into two sections, data pre-processing and model building.

## Data Pre-processingüîß

Let's look at the first 5 rows of the dataset -

<img src="{{ site.url }}{{ site.baseurl }}/images/diabetes_blog/data.PNG" alt="Data">

Our target variable will be Outcome which works as follows - 0 is false in binary which implies that the person does not have diabetes and 1 meaning true in binary which implies the person has diabetes.

The first task in data pre-processing is to handle missing values.

<img src="{{ site.url }}{{ site.baseurl }}/images/diabetes_blog/info.PNG" alt="Data Info">

But, there are no missing values in this dataset! Calm down, here comes the plot twist. Let's have a closer look at the dataset by executing -

```python
  df.describe()
```
<img src="{{ site.url }}{{ site.baseurl }}/images/diabetes_blog/describe.PNG" alt="Data desc">

Do you see what's wrong? Check the min values of each feature. Have you seen someone having a BloodPressure 0?! or someone with BMI equal to 0?! That's the catch. We need to solve this by replacing the 0s in the columns - Glucose, BloodPressure, SkinThickness, Insulin and BMI. I know that maximum values for pregnancies is 17 which looks shocking, we will deal with that in the outliers part.

There are many ways to replace the 0s, let me show you what I did. First, I made a new column called age_category which labels an age in a certain category. For now, you can think of it as a frequency table.

```python
  # make age category column which will help in filling values which are 0
  df['age_category'] = pd.cut(df['Age'], bins=[0, 21, 31, 41, 51, 61, 71, 81, np.inf], labels=[1,2,3,4,5,6,7,8])
  df['age_category'] = df['age_category'].astype(int)
```
After that, I filled the 0s by considering the average of all values present in the age_category in which that 0 belongs to. Here's the code -

```python
# Fill insulin column with respect to age category
  agecat = [1,2,3,4,5,6,7,8]

  for agec in agecat:
      df['Insulin'].replace(0, round(df[df['age_category'] == agec]['Insulin'].mean(), 0), inplace=True)
```
Similarly I did this for all features mentioned above. Here's the code for that -

```python
  # Fill rest with same strategy
  features = ['Glucose', 'BloodPressure', 'SkinThickness', 'BMI']
  for feature in features:
    for agec in agecat:
        if df[feature].dtype == 'float64':
            df[feature].replace(0, round(df[df['age_category'] == agec][feature].mean(), 1), inplace=True)
        else:
            df[feature].replace(0, round(df[df['age_category'] == agec][feature].mean(), 0), inplace=True)
```
As you can see, the minimum value of Insulin is 14 now, hence all 0s have been imputed.

<img src="{{ site.url }}{{ site.baseurl }}/images/diabetes_blog/desc_clean.PNG" alt="Data desc after cleaning">

## Outliers‚ö†Ô∏è

Look at the maximum values, they seem to be really really high and it's not possible to have maximum value of Insulin to be 846. Hence its an outlier. We can use box plots to confirm our assumptions of these outliers. Now, I used two techniques to remove outliers -

### 1. Z-Score

Z-Score describes any data point by finding their relationship between mean and standard deviation of the group of data points. It finds the distribution of data where mean is 0 and standard deviation is 1. While calculating Z-score we will re-scale and center the data and we will look for data points too far from 0, these are the outliers. Here, I have used a threshold of 3 or -3 (which is true in most cases), that is if Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outlier.

<img src="{{ site.url }}{{ site.baseurl }}/images/diabetes_blog/z-score.PNG" alt="Z-score">

To remove the outliers, refer to the following code snippet-

```python
  df = df[(z < 3).all(axis=1)]
```

### 2. IQR Score

IQR score is the difference between 75th and 25th percentiles that is upper and lower quartile. After doing the Z-score method, I still found outliers, so I decided to use IQR score method to remove them.

<img src="{{ site.url }}{{ site.baseurl }}/images/diabetes_blog/iqr_score.PNG" alt="IQR score">

This is the IQR score for each feature. Now, let's search for outliers. The below code will output some true false values. False means that these values are valid and True means they are an outlier.

<img src="{{ site.url }}{{ site.baseurl }}/images/diabetes_blog/remove_out_iqr.PNG" alt="Remove Outliers">

To filter out the outliers, refer to the code snippet below -

```python
  df = df[~((df < (Q1 - 1.5 * IQR)) | (df > (Q3 + 1.5 * IQR))).any(axis=1)]
```
Now, it's time for some ML!

## Model Buildingüö•

Before implementing any algorithm, we need to apply train_test_split on the dataset to get our train and test datasets. We will split the dataset as 80% train and 20% test.

```python
  from sklearn.model_selection import train_test_split
  X = df.drop(['Outcome'], axis=1)
  y = df['Outcome']
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
```
The stratify parameter is used so that the proportion of values in the sample produced in our test set will be the same as the proportion of values provided to parameter stratify.

### Logistic Regression

The most basic algorithm we could use, for a classification problem. Import LogisticRegression from sklearn.linear_model and fit it on the train set.

```python
  from sklearn.linear_model import LogisticRegression
  lr = LogisticRegression()
  lr.fit(X_train, y_train)
```
Predict on test data and output the confusion matrix and accuracy score after importing those two from sklearn.metrics

<img src="{{ site.url }}{{ site.baseurl }}/images/diabetes_blog/lr_predict.PNG" alt="Logistic Regression Prediction">

### Random Forest Classifier

Import RandomForestClassifier from sklearn.ensemble and fit it on the training set.

```python
  from sklearn.ensemble import RandomForestClassifier
  rfc = RandomForestClassifier()
  rfc.fit(X_train, y_train)
```
Predict on test data and output the confusion matrix and accuracy score after importing those two from sklearn.metrics

<img src="{{ site.url }}{{ site.baseurl }}/images/diabetes_blog/rfc_predict.PNG" alt="Random Forest Classifier Prediction">

### XGBoost Classifier

Import xgboost, declare a variable classifier which is equal to xgboost.XGBClassifier and fit it on the training set.

```python
  import xgboost
  xgbc = xgboost.XGBClassifier()
  xgbc.fit(X_train, y_train)
```
Predict on test data and output the confusion matrix and accuracy score after importing those two from sklearn.metrics

<img src="{{ site.url }}{{ site.baseurl }}/images/diabetes_blog/xgb_predict.PNG" alt="XGBoost Prediction">

## Conclusion

Accuracy of the three models -
1. Logistic Regression = 0.801
2. Random Forest Classifier = 0.819
3. XGBClassifier = 0.8108

Therefore, Random Forest Classifier has most accuracy in this case. There can be more work done that is, a better way of dealing with outliers or a better way to fill 0s. Do have a look at the [dataset](https://www.kaggle.com/uciml/pima-indians-diabetes-database) and the [code](https://github.com/devanshu125/PIMA-Indians-Diabetes-Database) and try to come up with your own approach to solve this. I have also built a [web app](https://pima-diabetes-predictor.herokuapp.com/) to predict diabetes. Do check that out!
